{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMGyuwuzqQrRn7bPGMdlgrE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"e_GTdBKYmJli"},"outputs":[],"source":["pip install faker sdv pandas"]},{"cell_type":"code","source":["# CELL 1: Setup and Imports\n","import pandas as pd\n","import numpy as np\n","from sdv.metadata import Metadata\n","from sdv.multi_table import HMASynthesizer\n","from sdv.single_table import GaussianCopulaSynthesizer, CTGANSynthesizer\n","from sdv.evaluation.multi_table import evaluate_quality\n","import networkx as nx\n","from typing import Dict, List, Tuple, Optional\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Set random seed for reproducibility\n","np.random.seed(42)\n","\n","print(\"üöÄ Advanced Multi-Table SDV Environment Ready!\")\n","print(\"üìä Target: 10 tables, 20+ columns each, 5-level depth relationships\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O68Htj6jmiBa","executionInfo":{"status":"ok","timestamp":1755678685809,"user_tz":-480,"elapsed":18519,"user":{"displayName":"Lakshminarayana Naidu","userId":"02129415072663551429"}},"outputId":"bbfee277-dedd-4111-a293-e913f05cfa9c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ Advanced Multi-Table SDV Environment Ready!\n","üìä Target: 10 tables, 20+ columns each, 5-level depth relationships\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"RA0gepDSn3zP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# CELL 2: Enhanced Recursive Multi-Table Synthesizer Class\n","# ============================================================================\n","\n","class AdvancedMultiTableSynthesizer:\n","    \"\"\"\n","    Enhanced multi-table synthetic data generator with deep relationship handling.\n","    Supports complex hierarchies, recursive relationships, and advanced validation.\n","    \"\"\"\n","\n","    def __init__(self, synthesizer_type='gaussian_copula', random_seed=42):\n","        \"\"\"Initialize the advanced multi-table synthesizer.\"\"\"\n","        self.synthesizer_type = synthesizer_type\n","        self.random_seed = random_seed\n","        self.metadata = Metadata()\n","        self.synthesizer = None\n","        self.table_dependencies = {}\n","        self.dependency_graph = nx.DiGraph()\n","        self.real_data = {}\n","        self.synthetic_data = {}\n","        self.relationships = []\n","        self.table_levels = {}  # Track hierarchy levels\n","\n","        np.random.seed(random_seed)\n","\n","    def add_table_with_level(self, table_name: str, data: pd.DataFrame,\n","                           primary_key: str, level: int = 0):\n","        \"\"\"Add a table with its hierarchy level.\"\"\"\n","        self.real_data[table_name] = data.copy()\n","        self.table_levels[table_name] = level\n","\n","        # Add to metadata\n","        self.metadata.detect_table_from_dataframe(table_name, data)\n","\n","        if primary_key and primary_key in data.columns:\n","            try:\n","                self.metadata.update_column(table_name, primary_key, sdtype='id')\n","                self.metadata.set_primary_key(table_name, primary_key)\n","                print(f\"‚úÖ Level {level}: Added '{table_name}' ({data.shape[0]} rows, {data.shape[1]} cols)\")\n","            except Exception as e:\n","                print(f\"‚ö†Ô∏è Warning setting primary key for {table_name}: {e}\")\n","\n","    def add_relationship_with_validation(self, child_table: str, child_column: str,\n","                                       parent_table: str, parent_column: str):\n","        \"\"\"Add relationship with comprehensive validation.\"\"\"\n","        try:\n","            # Validate tables exist\n","            if child_table not in self.real_data:\n","                raise ValueError(f\"Child table '{child_table}' not found\")\n","            if parent_table not in self.real_data:\n","                raise ValueError(f\"Parent table '{parent_table}' not found\")\n","\n","            # Validate columns exist\n","            child_df = self.real_data[child_table]\n","            parent_df = self.real_data[parent_table]\n","\n","            if child_column not in child_df.columns:\n","                raise ValueError(f\"Column '{child_column}' not found in {child_table}\")\n","            if parent_column not in parent_df.columns:\n","                raise ValueError(f\"Column '{parent_column}' not found in {parent_table}\")\n","\n","            # Validate referential integrity\n","            parent_values = set(parent_df[parent_column].dropna().unique())\n","            child_values = set(child_df[child_column].dropna().unique())\n","            invalid_refs = child_values - parent_values\n","\n","            if invalid_refs:\n","                print(f\"‚ö†Ô∏è Found {len(invalid_refs)} invalid references in relationship\")\n","                # Option to clean the data\n","                child_df_clean = child_df[child_df[child_column].isin(parent_values) |\n","                                        child_df[child_column].isna()]\n","                self.real_data[child_table] = child_df_clean\n","                print(f\"üîß Cleaned {child_table}: {len(child_df) - len(child_df_clean)} rows removed\")\n","\n","            # Set up relationship\n","            self.metadata.update_column(child_table, child_column, sdtype='id')\n","\n","            self.metadata.add_relationship(\n","                parent_table_name=parent_table,\n","                child_table_name=child_table,\n","                parent_primary_key=parent_column,\n","                child_foreign_key=child_column\n","            )\n","\n","            # Track dependencies\n","            self.dependency_graph.add_edge(parent_table, child_table)\n","\n","            if child_table not in self.table_dependencies:\n","                self.table_dependencies[child_table] = {}\n","            self.table_dependencies[child_table][child_column] = parent_table\n","\n","            self.relationships.append({\n","                'parent_table': parent_table,\n","                'parent_column': parent_column,\n","                'child_table': child_table,\n","                'child_column': child_column,\n","                'parent_level': self.table_levels.get(parent_table, 0),\n","                'child_level': self.table_levels.get(child_table, 0)\n","            })\n","\n","            parent_level = self.table_levels.get(parent_table, 0)\n","            child_level = self.table_levels.get(child_table, 0)\n","            print(f\"üîó L{parent_level}‚ÜíL{child_level}: {parent_table}.{parent_column} ‚Üí {child_table}.{child_column}\")\n","\n","        except Exception as e:\n","            print(f\"‚ùå Error adding relationship: {e}\")\n","\n","    def analyze_hierarchy(self):\n","        \"\"\"Analyze and display the relationship hierarchy.\"\"\"\n","        print(\"\\n\" + \"=\"*60)\n","        print(\"üìä RELATIONSHIP HIERARCHY ANALYSIS\")\n","        print(\"=\"*60)\n","\n","        # Group tables by level\n","        level_groups = {}\n","        for table, level in self.table_levels.items():\n","            if level not in level_groups:\n","                level_groups[level] = []\n","            level_groups[level].append(table)\n","\n","        # Display hierarchy\n","        for level in sorted(level_groups.keys()):\n","            tables = level_groups[level]\n","            print(f\"Level {level}: {', '.join(tables)}\")\n","\n","        # Show relationships by level\n","        print(\"\\nüîó Relationships by Level:\")\n","        for rel in sorted(self.relationships, key=lambda x: (x['parent_level'], x['child_level'])):\n","            print(f\"  L{rel['parent_level']}‚ÜíL{rel['child_level']}: \"\n","                  f\"{rel['parent_table']} ‚Üí {rel['child_table']}\")\n","\n","        # Calculate depth\n","        max_level = max(self.table_levels.values()) if self.table_levels else 0\n","        print(f\"\\nüìè Maximum Hierarchy Depth: {max_level + 1} levels\")\n","\n","        # Detect potential issues\n","        cycles = list(nx.simple_cycles(self.dependency_graph))\n","        if cycles:\n","            print(f\"‚ö†Ô∏è Circular dependencies detected: {cycles}\")\n","        else:\n","            print(\"‚úÖ No circular dependencies detected\")\n","\n","    def fit_advanced(self, validation_split=0.1):\n","        \"\"\"Advanced training with validation and monitoring.\"\"\"\n","        print(\"\\n\" + \"=\"*60)\n","        print(\"üß† ADVANCED MODEL TRAINING\")\n","        print(\"=\"*60)\n","\n","        # Validate metadata\n","        try:\n","            self.metadata.validate()\n","            print(\"‚úÖ Metadata validation successful\")\n","        except Exception as e:\n","            print(f\"‚ùå Metadata validation failed: {e}\")\n","            raise\n","\n","        # Initialize synthesizer\n","        print(f\"üöÄ Initializing HMA Synthesizer...\")\n","        self.synthesizer = HMASynthesizer(\n","            metadata=self.metadata,\n","\n","        )\n","\n","        print(\"üìä Training Data Summary:\")\n","        total_rows = sum(df.shape[0] for df in self.real_data.values())\n","        total_cols = sum(df.shape[1] for df in self.real_data.values())\n","        print(f\"  ‚Ä¢ Total rows across all tables: {total_rows:,}\")\n","        print(f\"  ‚Ä¢ Total columns across all tables: {total_cols}\")\n","        print(f\"  ‚Ä¢ Number of relationships: {len(self.relationships)}\")\n","\n","        # Train the model\n","        print(\"\\nüî• Starting training process...\")\n","        try:\n","            self.synthesizer.fit(self.real_data)\n","            print(\"‚úÖ Training completed successfully!\")\n","        except Exception as e:\n","            print(f\"‚ùå Training failed: {e}\")\n","            raise\n","\n","    def generate_advanced(self, scale_factor=1.0, custom_sizes=None):\n","        \"\"\"Advanced generation with monitoring and validation.\"\"\"\n","        print(\"\\n\" + \"=\"*60)\n","        print(\"‚ö° ADVANCED SYNTHETIC DATA GENERATION\")\n","        print(\"=\"*60)\n","\n","        if not self.synthesizer:\n","            raise ValueError(\"Model not trained. Call fit_advanced() first.\")\n","\n","        print(f\"üéØ Generation parameters:\")\n","        print(f\"  ‚Ä¢ Scale factor: {scale_factor}\")\n","        if custom_sizes:\n","            print(f\"  ‚Ä¢ Custom sizes: {custom_sizes}\")\n","\n","        try:\n","            if custom_sizes:\n","                self.synthetic_data = self.synthesizer.sample(num_rows=custom_sizes)\n","            else:\n","                self.synthetic_data = self.synthesizer.sample(scale=scale_factor)\n","\n","            print(\"‚úÖ Generation completed!\")\n","\n","            # Display results\n","            print(\"\\nüìà Generated Data Summary:\")\n","            for table_name, df in self.synthetic_data.items():\n","                level = self.table_levels.get(table_name, '?')\n","                print(f\"  L{level} {table_name}: {df.shape[0]:,} rows √ó {df.shape[1]} cols\")\n","\n","            return self.synthetic_data\n","\n","        except Exception as e:\n","            print(f\"‚ùå Generation failed: {e}\")\n","            raise\n","\n","    def comprehensive_validation(self):\n","        \"\"\"Comprehensive validation of synthetic data quality.\"\"\"\n","        print(\"\\n\" + \"=\"*60)\n","        print(\"üîç COMPREHENSIVE VALIDATION\")\n","        print(\"=\"*60)\n","\n","        if not self.synthetic_data:\n","            print(\"‚ùå No synthetic data to validate\")\n","            return {}\n","\n","        validation_results = {\n","            'referential_integrity': {},\n","            'data_quality': {},\n","            'statistical_similarity': {}\n","        }\n","\n","        # 1. Referential Integrity Check\n","        print(\"1Ô∏è‚É£ Checking referential integrity...\")\n","        for rel in self.relationships:\n","            rel_name = f\"{rel['parent_table']} ‚Üí {rel['child_table']}\"\n","            parent_df = self.synthetic_data[rel['parent_table']]\n","            child_df = self.synthetic_data[rel['child_table']]\n","\n","            parent_values = set(parent_df[rel['parent_column']].dropna().unique())\n","            child_fk_values = set(child_df[rel['child_column']].dropna().unique())\n","\n","            invalid_refs = child_fk_values - parent_values\n","            is_valid = len(invalid_refs) == 0\n","\n","            validation_results['referential_integrity'][rel_name] = {\n","                'valid': is_valid,\n","                'invalid_count': len(invalid_refs),\n","                'integrity_ratio': (len(child_fk_values) - len(invalid_refs)) / len(child_fk_values) if child_fk_values else 1.0\n","            }\n","\n","            status = \"‚úÖ\" if is_valid else f\"‚ùå ({len(invalid_refs)} invalid)\"\n","            print(f\"  {rel_name}: {status}\")\n","\n","        # 2. Data Quality Checks\n","        print(\"\\n2Ô∏è‚É£ Checking data quality...\")\n","        for table_name in self.real_data.keys():\n","            real_df = self.real_data[table_name]\n","            synthetic_df = self.synthetic_data[table_name]\n","\n","            # Basic quality metrics\n","            quality_metrics = {\n","                'shape_match': real_df.shape[1] == synthetic_df.shape[1],\n","                'null_percentage_real': (real_df.isnull().sum().sum() / real_df.size) * 100,\n","                'null_percentage_synthetic': (synthetic_df.isnull().sum().sum() / synthetic_df.size) * 100,\n","                'dtypes_match': len(set(real_df.dtypes) & set(synthetic_df.dtypes)) == len(set(real_df.dtypes))\n","            }\n","\n","            validation_results['data_quality'][table_name] = quality_metrics\n","\n","            print(f\"  {table_name}:\")\n","            print(f\"    Shape match: {'‚úÖ' if quality_metrics['shape_match'] else '‚ùå'}\")\n","            print(f\"    Null %: Real {quality_metrics['null_percentage_real']:.1f}%, \"\n","                  f\"Synthetic {quality_metrics['null_percentage_synthetic']:.1f}%\")\n","\n","        return validation_results\n","\n","    def export_results(self, output_dir=\"synthetic_output\"):\n","        \"\"\"Export all synthetic data and reports.\"\"\"\n","        import os\n","\n","        if not os.path.exists(output_dir):\n","            os.makedirs(output_dir)\n","\n","        print(f\"\\nüíæ Exporting results to '{output_dir}'...\")\n","\n","        # Export synthetic data\n","        for table_name, df in self.synthetic_data.items():\n","            filepath = os.path.join(output_dir, f\"{table_name}_synthetic.csv\")\n","            df.to_csv(filepath, index=False)\n","            print(f\"  Exported {table_name}: {filepath}\")\n","\n","        # Export metadata\n","        metadata_path = os.path.join(output_dir, \"metadata.json\")\n","        self.metadata.save(metadata_path)\n","        print(f\"  Exported metadata: {metadata_path}\")\n","\n","        # Export relationship summary\n","        summary_path = os.path.join(output_dir, \"relationship_summary.txt\")\n","        with open(summary_path, 'w') as f:\n","            f.write(\"RELATIONSHIP HIERARCHY SUMMARY\\n\")\n","            f.write(\"=\"*50 + \"\\n\\n\")\n","            for rel in self.relationships:\n","                f.write(f\"L{rel['parent_level']}‚ÜíL{rel['child_level']}: \"\n","                       f\"{rel['parent_table']}.{rel['parent_column']} ‚Üí \"\n","                       f\"{rel['child_table']}.{rel['child_column']}\\n\")\n","\n","        print(f\"‚úÖ Export completed!\")"],"metadata":{"id":"dE7gdGDrmwLQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# CELL 3: Create Complex 10-Table Dataset with 5-Level Hierarchy\n","# ============================================================================\n","\n","def create_complex_enterprise_data():\n","    \"\"\"\n","    Create a comprehensive 10-table enterprise dataset with 5-level hierarchy.\n","    Each table has 20+ columns representing realistic business scenarios.\n","    \"\"\"\n","    print(\"\\nüèóÔ∏è Creating Complex Enterprise Dataset...\")\n","    print(\"üìä Target: 10 tables, 20+ columns each, 5-level depth\")\n","\n","    # LEVEL 0: Root Tables (No dependencies)\n","\n","    # Table 1: Companies (Root level)\n","    print(\"Creating Level 0 tables...\")\n","    companies = pd.DataFrame({\n","        'company_id': range(1, 51),  # 50 companies\n","        'company_name': [f'Company_{i}' for i in range(1, 51)],\n","        'industry': np.random.choice(['Technology', 'Healthcare', 'Finance', 'Manufacturing', 'Retail'], 50),\n","        'founding_year': np.random.randint(1980, 2020, 50),\n","        'headquarters_country': np.random.choice(['USA', 'Canada', 'UK', 'Germany', 'Japan'], 50),\n","        'headquarters_city': np.random.choice(['New York', 'London', 'Tokyo', 'Berlin', 'Toronto'], 50),\n","        'employee_count': np.random.randint(100, 50000, 50),\n","        'annual_revenue': np.random.uniform(1_000_000, 1_000_000_000, 50),\n","        'market_cap': np.random.uniform(10_000_000, 10_000_000_000, 50),\n","        'ceo_name': [f'CEO_{i}' for i in range(1, 51)],\n","        'is_public': np.random.choice([True, False], 50),\n","        'stock_symbol': [f'SYM{i:02d}' if np.random.random() > 0.3 else None for i in range(1, 51)],\n","        'website': [f'www.company{i}.com' for i in range(1, 51)],\n","        'email_domain': [f'company{i}.com' for i in range(1, 51)],\n","        'phone': [f'+1-555-{i:04d}' for i in range(1000, 1050)],\n","        'business_model': np.random.choice(['B2B', 'B2C', 'B2B2C', 'Marketplace'], 50),\n","        'primary_product': [f'Product_Line_{i}' for i in range(1, 51)],\n","        'sustainability_score': np.random.uniform(1, 10, 50),\n","        'innovation_index': np.random.uniform(1, 100, 50),\n","        'risk_rating': np.random.choice(['Low', 'Medium', 'High'], 50),\n","        'last_funding_round': np.random.choice(['Seed', 'Series A', 'Series B', 'Series C', 'IPO', None], 50),\n","        'total_funding': np.random.uniform(0, 500_000_000, 50),\n","        'created_at': pd.date_range('2020-01-01', periods=50, freq='W'),\n","        'updated_at': pd.date_range('2024-01-01', periods=50, freq='D')\n","    })\n","\n","    # Table 2: Geographic Regions (Root level)\n","    regions = pd.DataFrame({\n","        'region_id': range(1, 26),  # 25 regions\n","        'region_name': [f'Region_{i}' for i in range(1, 26)],\n","        'country': np.random.choice(['USA', 'Canada', 'UK', 'Germany', 'Japan', 'Australia'], 25),\n","        'continent': np.random.choice(['North America', 'Europe', 'Asia', 'Oceania'], 25),\n","        'timezone': np.random.choice(['UTC-8', 'UTC-5', 'UTC+0', 'UTC+1', 'UTC+9'], 25),\n","        'currency': np.random.choice(['USD', 'EUR', 'GBP', 'JPY', 'CAD'], 25),\n","        'population': np.random.randint(100_000, 50_000_000, 25),\n","        'gdp_per_capita': np.random.uniform(20_000, 80_000, 25),\n","        'unemployment_rate': np.random.uniform(2, 15, 25),\n","        'inflation_rate': np.random.uniform(-1, 8, 25),\n","        'cost_of_living_index': np.random.uniform(50, 150, 25),\n","        'ease_of_business_rank': np.random.randint(1, 200, 25),\n","        'tax_rate': np.random.uniform(15, 45, 25),\n","        'language_primary': np.random.choice(['English', 'German', 'Japanese', 'French'], 25),\n","        'internet_penetration': np.random.uniform(60, 98, 25),\n","        'smartphone_penetration': np.random.uniform(50, 95, 25),\n","        'education_index': np.random.uniform(0.5, 1.0, 25),\n","        'healthcare_index': np.random.uniform(0.4, 0.9, 25),\n","        'climate_type': np.random.choice(['Temperate', 'Tropical', 'Arid', 'Continental'], 25),\n","        'average_temperature': np.random.uniform(-5, 35, 25),\n","        'renewable_energy_percentage': np.random.uniform(10, 80, 25),\n","        'carbon_footprint_per_capita': np.random.uniform(2, 20, 25),\n","        'created_at': pd.date_range('2020-01-01', periods=25, freq='2W'),\n","        'updated_at': pd.date_range('2024-01-01', periods=25, freq='3D')\n","    })\n","\n","    # LEVEL 1: Tables depending on Level 0\n","\n","    # Table 3: Departments (depends on Companies)\n","    print(\"Creating Level 1 tables...\")\n","    departments = pd.DataFrame({\n","        'department_id': range(1, 201),  # 200 departments\n","        'company_id': np.random.choice(companies['company_id'], 200),\n","        'department_name': np.random.choice(['Engineering', 'Sales', 'Marketing', 'HR', 'Finance', 'Operations'], 200),\n","        'department_code': [f'DEPT_{i:03d}' for i in range(1, 201)],\n","        'head_of_department': [f'Manager_{i}' for i in range(1, 201)],\n","        'budget_annual': np.random.uniform(100_000, 10_000_000, 200),\n","        'budget_used': np.random.uniform(50_000, 8_000_000, 200),\n","        'employee_count': np.random.randint(5, 500, 200),\n","        'performance_score': np.random.uniform(1, 10, 200),\n","        'establishment_date': pd.date_range('2015-01-01', periods=200, freq='W'),\n","        'office_location': np.random.choice(['Floor 1', 'Floor 2', 'Floor 3', 'Building A', 'Building B'], 200),\n","        'cost_center': [f'CC_{i:04d}' for i in range(1000, 1200)],\n","        'profit_center': [f'PC_{i:04d}' for i in range(2000, 2200)],\n","        'functional_area': np.random.choice(['Core', 'Support', 'Strategic', 'Operational'], 200),\n","        'automation_level': np.random.uniform(0, 100, 200),\n","        'digital_maturity': np.random.choice(['Basic', 'Intermediate', 'Advanced', 'Expert'], 200),\n","        'collaboration_score': np.random.uniform(1, 10, 200),\n","        'innovation_projects': np.random.randint(0, 20, 200),\n","        'training_hours_annual': np.random.randint(20, 200, 200),\n","        'employee_satisfaction': np.random.uniform(1, 10, 200),\n","        'turnover_rate': np.random.uniform(5, 25, 200),\n","        'diversity_index': np.random.uniform(0.3, 0.9, 200),\n","        'created_at': pd.date_range('2020-01-01', periods=200, freq='D'),\n","        'updated_at': pd.date_range('2024-01-01', periods=200, freq='12H')\n","    })\n","\n","    # Table 4: Office Locations (depends on Companies and Regions)\n","    offices = pd.DataFrame({\n","        'office_id': range(1, 151),  # 150 offices\n","        'company_id': np.random.choice(companies['company_id'], 150),\n","        'region_id': np.random.choice(regions['region_id'], 150),\n","        'office_name': [f'Office_{i}' for i in range(1, 151)],\n","        'office_type': np.random.choice(['Headquarters', 'Branch', 'Subsidiary', 'Co-working'], 150),\n","        'address_line1': [f'{i} Business Street' for i in range(1, 151)],\n","        'address_line2': [f'Suite {i}00' if np.random.random() > 0.3 else None for i in range(1, 151)],\n","        'postal_code': [f'{i:05d}' for i in range(10000, 10150)],\n","        'phone_number': [f'+1-555-{i:04d}' for i in range(2000, 2150)],\n","        'email': [f'office{i}@company.com' for i in range(1, 151)],\n","        'square_footage': np.random.randint(1000, 100000, 150),\n","        'max_capacity': np.random.randint(50, 2000, 150),\n","        'current_occupancy': np.random.randint(20, 1500, 150),\n","        'lease_type': np.random.choice(['Owned', 'Leased', 'Co-shared'], 150),\n","        'lease_expiry': pd.date_range('2025-01-01', periods=150, freq='M'),\n","        'monthly_rent': np.random.uniform(5000, 200000, 150),\n","        'utilities_cost': np.random.uniform(500, 10000, 150),\n","        'security_level': np.random.choice(['Basic', 'Standard', 'High', 'Maximum'], 150),\n","        'parking_spaces': np.random.randint(10, 500, 150),\n","        'accessibility_features': np.random.choice([True, False], 150),\n","        'green_certification': np.random.choice(['LEED Gold', 'LEED Silver', 'BREEAM', None], 150),\n","        'internet_speed_mbps': np.random.randint(100, 10000, 150),\n","        'created_at': pd.date_range('2019-01-01', periods=150, freq='3D'),\n","        'updated_at': pd.date_range('2024-01-01', periods=150, freq='W')\n","    })\n","\n","    # LEVEL 2: Tables depending on Level 1\n","\n","    # Table 5: Employees (depends on Departments and Offices)\n","    print(\"Creating Level 2 tables...\")\n","    employees = pd.DataFrame({\n","        'employee_id': range(1, 1001),  # 1000 employees\n","        'department_id': np.random.choice(departments['department_id'], 1000),\n","        'office_id': np.random.choice(offices['office_id'], 1000),\n","        'employee_number': [f'EMP_{i:06d}' for i in range(100000, 101000)],\n","        'first_name': [f'FirstName_{i}' for i in range(1, 1001)],\n","        'last_name': [f'LastName_{i}' for i in range(1, 1001)],\n","        'email': [f'employee{i}@company.com' for i in range(1, 1001)],\n","        'phone': [f'+1-555-{i:04d}' for i in range(3000, 4000)],\n","        'hire_date': pd.date_range('2018-01-01', periods=1000, freq='D'),\n","        'birth_date': pd.date_range('1970-01-01', periods=1000, freq='3D'),\n","        'gender': np.random.choice(['Male', 'Female', 'Other'], 1000),\n","        'job_title': np.random.choice(['Developer', 'Manager', 'Analyst', 'Designer', 'Specialist'], 1000),\n","        'job_level': np.random.choice(['Junior', 'Mid', 'Senior', 'Lead', 'Director'], 1000),\n","        'employment_type': np.random.choice(['Full-time', 'Part-time', 'Contract', 'Intern'], 1000),\n","        'salary_annual': np.random.uniform(40000, 200000, 1000),\n","        'bonus_percentage': np.random.uniform(0, 25, 1000),\n","        'stock_options': np.random.randint(0, 10000, 1000),\n","        'performance_rating': np.random.uniform(1, 10, 1000),\n","        'education_level': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], 1000),\n","        'years_experience': np.random.randint(0, 30, 1000),\n","        'skill_level': np.random.uniform(1, 10, 1000),\n","        'training_completed': np.random.randint(0, 50, 1000),\n","        'last_promotion_date': pd.date_range('2020-01-01', periods=1000, freq='2M'),\n","        'created_at': pd.date_range('2018-01-01', periods=1000, freq='D'),\n","        'updated_at': pd.date_range('2024-01-01', periods=1000, freq='6H')\n","    })\n","\n","    # Table 6: Projects (depends on Departments)\n","    projects = pd.DataFrame({\n","        'project_id': range(1, 301),  # 300 projects\n","        'department_id': np.random.choice(departments['department_id'], 300),\n","        'project_name': [f'Project_{i}' for i in range(1, 301)],\n","        'project_code': [f'PRJ_{i:04d}' for i in range(1, 301)],\n","        'project_type': np.random.choice(['Internal', 'Client', 'Research', 'Maintenance'], 300),\n","        'status': np.random.choice(['Planning', 'Active', 'On Hold', 'Completed', 'Cancelled'], 300),\n","        'priority': np.random.choice(['Low', 'Medium', 'High', 'Critical'], 300),\n","        'start_date': pd.date_range('2022-01-01', periods=300, freq='3D'),\n","        'end_date': pd.date_range('2024-01-01', periods=300, freq='W'),\n","        'budget_allocated': np.random.uniform(10000, 5000000, 300),\n","        'budget_spent': np.random.uniform(5000, 4000000, 300),\n","        'progress_percentage': np.random.uniform(0, 100, 300),\n","        'project_manager': [f'PM_{i}' for i in range(1, 301)],\n","        'client_name': [f'Client_{i}' if np.random.random() > 0.4 else None for i in range(1, 301)],\n","        'team_size': np.random.randint(2, 50, 300),\n","        'risk_level': np.random.choice(['Low', 'Medium', 'High'], 300),\n","        'methodology': np.random.choice(['Agile', 'Waterfall', 'Hybrid', 'Kanban'], 300),\n","        'technology_stack': np.random.choice(['Python', 'Java', 'JavaScript', 'C#', 'Go'], 300),\n","        'complexity_score': np.random.uniform(1, 10, 300),\n","        'deliverables_count': np.random.randint(1, 20, 300),\n","        'stakeholder_count': np.random.randint(2, 15, 300),\n","        'communication_frequency': np.random.choice(['Daily', 'Weekly', 'Bi-weekly', 'Monthly'], 300),\n","        'quality_score': np.random.uniform(1, 10, 300),\n","        'customer_satisfaction': np.random.uniform(1, 10, 300),\n","        'created_at': pd.date_range('2022-01-01', periods=300, freq='2D'),\n","        'updated_at': pd.date_range('2024-01-01', periods=300, freq='8H')\n","    })\n","\n","    # LEVEL 3: Tables depending on Level 2\n","\n","    # Table 7: Tasks (depends on Projects and Employees)\n","    print(\"Creating Level 3 tables...\")\n","    tasks = pd.DataFrame({\n","        'task_id': range(1, 1501),  # 1500 tasks\n","        'project_id': np.random.choice(projects['project_id'], 1500),\n","        'assigned_employee_id': np.random.choice(employees['employee_id'], 1500),\n","        'task_name': [f'Task_{i}' for i in range(1, 1501)],\n","        'task_description': [f'Description for task {i}' for i in range(1, 1501)],\n","        'task_type': np.random.choice(['Development', 'Testing', 'Design', 'Documentation', 'Review'], 1500),\n","        'priority': np.random.choice(['Low', 'Medium', 'High', 'Urgent'], 1500),\n","        'status': np.random.choice(['Not Started', 'In Progress', 'Testing', 'Completed', 'Blocked'], 1500),\n","        'estimated_hours': np.random.uniform(1, 80, 1500),\n","        'actual_hours': np.random.uniform(1, 100, 1500),\n","        'start_date': pd.date_range('2022-06-01', periods=1500, freq='D'),\n","        'due_date': pd.date_range('2024-01-01', periods=1500, freq='D'),\n","        'completion_date': pd.date_range('2023-01-01', periods=1500, freq='2D'),\n","        'difficulty_level': np.random.uniform(1, 10, 1500),\n","        'quality_rating': np.random.uniform(1, 10, 1500),\n","        'dependencies_count': np.random.randint(0, 10, 1500),\n","        'milestone_id': np.random.randint(1, 100, 1500),\n","        'story_points': np.random.choice([1, 2, 3, 5, 8, 13], 1500),\n","        'sprint_number': np.random.randint(1, 20, 1500),\n","        'tags': [f'tag{i},tag{i+1}' for i in range(1, 1501)],\n","        'comments_count': np.random.randint(0, 50, 1500),\n","        'attachments_count': np.random.randint(0, 10, 1500),\n","        'review_required': np.random.choice([True, False], 1500),\n","        'created_at': pd.date_range('2022-06-01', periods=1500, freq='6H'),\n","        'updated_at': pd.date_range('2024-01-01', periods=1500, freq='3H')\n","    })\n","\n","    # Table 8: Performance Reviews (depends on Employees)\n","    performance_reviews = pd.DataFrame({\n","        'review_id': range(1, 501),  # 500 reviews\n","        'employee_id': np.random.choice(employees['employee_id'], 500),\n","        'reviewer_employee_id': np.random.choice(employees['employee_id'], 500),\n","        'review_period': np.random.choice(['Q1', 'Q2', 'Q3', 'Q4', 'Annual', 'Mid-year'], 500),\n","        'review_year': np.random.choice([2022, 2023, 2024], 500),\n","        'review_type': np.random.choice(['Self', 'Manager', '360', 'Peer'], 500),\n","        'overall_rating': np.random.uniform(1, 10, 500),\n","        'goals_achievement': np.random.uniform(1, 10, 500),\n","        'technical_skills': np.random.uniform(1, 10, 500),\n","        'communication_skills': np.random.uniform(1, 10, 500),\n","        'leadership_skills': np.random.uniform(1, 10, 500),\n","        'teamwork_rating': np.random.uniform(1, 10, 500),\n","        'innovation_score': np.random.uniform(1, 10, 500),\n","        'problem_solving': np.random.uniform(1, 10, 500),\n","        'attendance_score': np.random.uniform(1, 10, 500),\n","        'punctuality_score': np.random.uniform(1, 10, 500),\n","        'goal_setting_next': [f'Goal_{i}' for i in range(1, 501)],\n","        'development_areas': [f'Development area {i}' for i in range(1, 501)],\n","        'strengths': [f'Strength {i}' for i in range(1, 501)],\n","        'promotion_readiness': np.random.choice(['Not Ready', 'Developing', 'Ready', 'Highly Ready'], 500),\n","        'salary_increase_recommended': np.random.uniform(0, 20, 500),\n","        'bonus_recommended': np.random.uniform(0, 50, 500),\n","        'training_recommendations': [f'Training {i}' for i in range(1, 501)],\n","        'created_at': pd.date_range('2022-01-01', periods=500, freq='W'),\n","        'updated_at': pd.date_range('2024-01-01', periods=500, freq='2D')\n","    })\n","\n","    # LEVEL 4: Tables depending on Level 3\n","\n","    # Table 9: Time Logs (depends on Tasks)\n","    print(\"Creating Level 4 tables...\")\n","    time_logs = pd.DataFrame({\n","        'log_id': range(1, 2001),  # 2000 time logs\n","        'task_id': np.random.choice(tasks['task_id'], 2000),\n","        'employee_id': np.random.choice(employees['employee_id'], 2000),\n","        'log_date': pd.date_range('2023-01-01', periods=2000, freq='6H'),\n","        'start_time': pd.date_range('2023-01-01 09:00:00', periods=2000, freq='3H'),\n","        'end_time': pd.date_range('2023-01-01 17:00:00', periods=2000, freq='3H'),\n","        'hours_logged': np.random.uniform(0.5, 8, 2000),\n","        'activity_type': np.random.choice(['Coding', 'Testing', 'Debugging', 'Meeting', 'Research'], 2000),\n","        'description': [f'Work description {i}' for i in range(1, 2001)],\n","        'location': np.random.choice(['Office', 'Home', 'Client Site', 'Co-working'], 2000),\n","        'productivity_rating': np.random.uniform(1, 10, 2000),\n","        'mood_rating': np.random.uniform(1, 10, 2000),\n","        'energy_level': np.random.uniform(1, 10, 2000),\n","        'interruptions_count': np.random.randint(0, 20, 2000),\n","        'collaboration_time': np.random.uniform(0, 4, 2000),\n","        'focus_time': np.random.uniform(0, 8, 2000),\n","        'tools_used': [f'Tool{i},Tool{i+1}' for i in range(1, 2001)],\n","        'issues_encountered': [f'Issue {i}' if np.random.random() > 0.7 else None for i in range(1, 2001)],\n","        'solutions_implemented': [f'Solution {i}' if np.random.random() > 0.8 else None for i in range(1, 2001)],\n","        'learning_points': [f'Learning {i}' for i in range(1, 2001)],\n","        'billable_hours': np.random.uniform(0, 8, 2000),\n","        'approval_status': np.random.choice(['Pending', 'Approved', 'Rejected'], 2000),\n","        'approved_by': [f'Manager_{i}' if np.random.random() > 0.2 else None for i in range(1, 2001)],\n","        'created_at': pd.date_range('2023-01-01', periods=2000, freq='2H'),\n","        'updated_at': pd.date_range('2024-01-01', periods=2000, freq='1H')\n","    })\n","\n","    # LEVEL 5: Tables depending on Level 4 (Final depth level)\n","\n","    # Table 10: Activity Details (depends on Time Logs) - Recursive relationship\n","    print(\"Creating Level 5 tables...\")\n","    activity_details = pd.DataFrame({\n","        'activity_id': range(1, 1001),  # 1000 activity details\n","        'log_id': np.random.choice(time_logs['log_id'], 1000),\n","        'parent_activity_id': [None] * 700 + list(np.random.choice(range(1, 301), 300)),  # Recursive!\n","        'activity_name': [f'Activity_{i}' for i in range(1, 1001)],\n","        'activity_category': np.random.choice(['Core Work', 'Communication', 'Learning', 'Admin', 'Break'], 1000),\n","        'activity_subcategory': np.random.choice(['Code Review', 'Email', 'Meeting', 'Documentation', 'Planning'], 1000),\n","        'start_time': pd.date_range('2023-01-01 09:00:00', periods=1000, freq='30min'),\n","        'end_time': pd.date_range('2023-01-01 17:00:00', periods=1000, freq='30min'),\n","        'duration_minutes': np.random.randint(5, 240, 1000),\n","        'complexity_level': np.random.uniform(1, 10, 1000),\n","        'completion_percentage': np.random.uniform(0, 100, 1000),\n","        'quality_score': np.random.uniform(1, 10, 1000),\n","        'effort_required': np.random.uniform(1, 10, 1000),\n","        'concentration_level': np.random.uniform(1, 10, 1000),\n","        'stress_level': np.random.uniform(1, 10, 1000),\n","        'satisfaction_level': np.random.uniform(1, 10, 1000),\n","        'collaboration_involved': np.random.choice([True, False], 1000),\n","        'tools_specific': [f'SpecificTool_{i}' for i in range(1, 1001)],\n","        'resources_used': [f'Resource_{i}' for i in range(1, 1001)],\n","        'obstacles_faced': [f'Obstacle {i}' if np.random.random() > 0.6 else None for i in range(1, 1001)],\n","        'outcomes_achieved': [f'Outcome {i}' for i in range(1, 1001)],\n","        'knowledge_gained': [f'Knowledge {i}' for i in range(1, 1001)],\n","        'follow_up_required': np.random.choice([True, False], 1000),\n","        'impact_score': np.random.uniform(1, 10, 1000),\n","        'created_at': pd.date_range('2023-01-01', periods=1000, freq='1H'),\n","        'updated_at': pd.date_range('2024-01-01', periods=1000, freq='30min')\n","    })\n","\n","    print(\"‚úÖ Complex Enterprise Dataset Created!\")\n","    print(f\"üìä Dataset Summary:\")\n","    print(f\"  ‚Ä¢ Level 0: Companies ({companies.shape[0]} rows), Regions ({regions.shape[0]} rows)\")\n","    print(f\"  ‚Ä¢ Level 1: Departments ({departments.shape[0]} rows), Offices ({offices.shape[0]} rows)\")\n","    print(f\"  ‚Ä¢ Level 2: Employees ({employees.shape[0]} rows), Projects ({projects.shape[0]} rows)\")\n","    print(f\"  ‚Ä¢ Level 3: Tasks ({tasks.shape[0]} rows), Performance Reviews ({performance_reviews.shape[0]} rows)\")\n","    print(f\"  ‚Ä¢ Level 4: Time Logs ({time_logs.shape[0]} rows)\")\n","    print(f\"  ‚Ä¢ Level 5: Activity Details ({activity_details.shape[0]} rows) - With Recursive Relationships!\")\n","\n","    total_rows = sum([df.shape[0] for df in [companies, regions, departments, offices, employees,\n","                     projects, tasks, performance_reviews, time_logs, activity_details]])\n","    total_cols = sum([df.shape[1] for df in [companies, regions, departments, offices, employees,\n","                     projects, tasks, performance_reviews, time_logs, activity_details]])\n","\n","    print(f\"üìà Total: {total_rows:,} rows across {total_cols} columns\")\n","\n","    return {\n","        'companies': companies,\n","        'regions': regions,\n","        'departments': departments,\n","        'offices': offices,\n","        'employees': employees,\n","        'projects': projects,\n","        'tasks': tasks,\n","        'performance_reviews': performance_reviews,\n","        'time_logs': time_logs,\n","        'activity_details': activity_details\n","    }"],"metadata":{"id":"E_TsAX0Em7Yw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# CELL 4: Initialize and Setup Complex Multi-Table Synthesizer\n","# ============================================================================\n","\n","# Create the complex dataset\n","print(\"üöÄ CREATING COMPLEX ENTERPRISE DATASET\")\n","print(\"=\"*60)\n","enterprise_data = create_complex_enterprise_data()\n","\n","# Initialize the advanced synthesizer\n","print(\"\\nüß† INITIALIZING ADVANCED MULTI-TABLE SYNTHESIZER\")\n","print(\"=\"*60)\n","synthesizer = AdvancedMultiTableSynthesizer(\n","    synthesizer_type='gaussian_copula',\n","    random_seed=42\n",")\n","\n","# Add all tables with their hierarchy levels\n","print(\"\\nAdding tables to synthesizer...\")\n","\n","# Level 0 (Root tables)\n","synthesizer.add_table_with_level('companies', enterprise_data['companies'], 'company_id', level=0)\n","synthesizer.add_table_with_level('regions', enterprise_data['regions'], 'region_id', level=0)\n","\n","# Level 1\n","synthesizer.add_table_with_level('departments', enterprise_data['departments'], 'department_id', level=1)\n","synthesizer.add_table_with_level('offices', enterprise_data['offices'], 'office_id', level=1)\n","\n","# Level 2\n","synthesizer.add_table_with_level('employees', enterprise_data['employees'], 'employee_id', level=2)\n","synthesizer.add_table_with_level('projects', enterprise_data['projects'], 'project_id', level=2)\n","\n","# Level 3\n","synthesizer.add_table_with_level('tasks', enterprise_data['tasks'], 'task_id', level=3)\n","synthesizer.add_table_with_level('performance_reviews', enterprise_data['performance_reviews'], 'review_id', level=3)\n","\n","# Level 4\n","synthesizer.add_table_with_level('time_logs', enterprise_data['time_logs'], 'log_id', level=4)\n","\n","# Level 5\n","synthesizer.add_table_with_level('activity_details', enterprise_data['activity_details'], 'activity_id', level=5)\n","\n","print(\"‚úÖ All tables added successfully!\")\n","\n","# ============================================================================\n","# CELL 5: Define Complex 5-Level Relationships\n","# ============================================================================\n","\n","print(\"\\nüîó DEFINING 5-LEVEL RELATIONSHIP HIERARCHY\")\n","print(\"=\"*60)\n","\n","# Level 0 ‚Üí Level 1 relationships\n","print(\"Setting up Level 0 ‚Üí Level 1 relationships...\")\n","synthesizer.add_relationship_with_validation('departments', 'company_id', 'companies', 'company_id')\n","synthesizer.add_relationship_with_validation('offices', 'company_id', 'companies', 'company_id')\n","synthesizer.add_relationship_with_validation('offices', 'region_id', 'regions', 'region_id')\n","\n","# Level 1 ‚Üí Level 2 relationships\n","print(\"\\nSetting up Level 1 ‚Üí Level 2 relationships...\")\n","synthesizer.add_relationship_with_validation('employees', 'department_id', 'departments', 'department_id')\n","synthesizer.add_relationship_with_validation('employees', 'office_id', 'offices', 'office_id')\n","synthesizer.add_relationship_with_validation('projects', 'department_id', 'departments', 'department_id')\n","\n","# Level 2 ‚Üí Level 3 relationships\n","print(\"\\nSetting up Level 2 ‚Üí Level 3 relationships...\")\n","synthesizer.add_relationship_with_validation('tasks', 'project_id', 'projects', 'project_id')\n","synthesizer.add_relationship_with_validation('tasks', 'assigned_employee_id', 'employees', 'employee_id')\n","synthesizer.add_relationship_with_validation('performance_reviews', 'employee_id', 'employees', 'employee_id')\n","synthesizer.add_relationship_with_validation('performance_reviews', 'reviewer_employee_id', 'employees', 'employee_id')\n","\n","# Level 3 ‚Üí Level 4 relationships\n","print(\"\\nSetting up Level 3 ‚Üí Level 4 relationships...\")\n","synthesizer.add_relationship_with_validation('time_logs', 'task_id', 'tasks', 'task_id')\n","synthesizer.add_relationship_with_validation('time_logs', 'employee_id', 'employees', 'employee_id')\n","\n","# Level 4 ‚Üí Level 5 relationships\n","print(\"\\nSetting up Level 4 ‚Üí Level 5 relationships...\")\n","synthesizer.add_relationship_with_validation('activity_details', 'log_id', 'time_logs', 'log_id')\n","\n","# RECURSIVE RELATIONSHIP (Level 5 ‚Üí Level 5)\n","print(\"\\nSetting up RECURSIVE relationship...\")\n","print(\"Adding self-referencing relationship in activity_details...\")\n","try:\n","    # Handle the recursive relationship manually since it's within the same table\n","    synthesizer.metadata.update_column('activity_details', 'parent_activity_id', sdtype='id')\n","    synthesizer.metadata.add_relationship(\n","        parent_table_name='activity_details',\n","        child_table_name='activity_details',\n","        parent_primary_key='activity_id',\n","        child_foreign_key='parent_activity_id'\n","    )\n","    print(\"‚úÖ Recursive relationship added: activity_details.activity_id ‚Üí activity_details.parent_activity_id\")\n","except Exception as e:\n","    print(f\"‚ö†Ô∏è Recursive relationship warning: {e}\")\n","\n","# Analyze the complete hierarchy\n","synthesizer.analyze_hierarchy()\n","\n","print(\"‚úÖ All relationships defined successfully!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PW-gqvkcnTCI","executionInfo":{"status":"ok","timestamp":1755678853729,"user_tz":-480,"elapsed":333,"user":{"displayName":"Lakshminarayana Naidu","userId":"02129415072663551429"}},"outputId":"aeb7f8cd-0729-4763-c5c9-257e121de476"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üöÄ CREATING COMPLEX ENTERPRISE DATASET\n","============================================================\n","\n","üèóÔ∏è Creating Complex Enterprise Dataset...\n","üìä Target: 10 tables, 20+ columns each, 5-level depth\n","Creating Level 0 tables...\n","Creating Level 1 tables...\n","Creating Level 2 tables...\n","Creating Level 3 tables...\n","Creating Level 4 tables...\n","Creating Level 5 tables...\n","‚úÖ Complex Enterprise Dataset Created!\n","üìä Dataset Summary:\n","  ‚Ä¢ Level 0: Companies (50 rows), Regions (25 rows)\n","  ‚Ä¢ Level 1: Departments (200 rows), Offices (150 rows)\n","  ‚Ä¢ Level 2: Employees (1000 rows), Projects (300 rows)\n","  ‚Ä¢ Level 3: Tasks (1500 rows), Performance Reviews (500 rows)\n","  ‚Ä¢ Level 4: Time Logs (2000 rows)\n","  ‚Ä¢ Level 5: Activity Details (1000 rows) - With Recursive Relationships!\n","üìà Total: 6,725 rows across 248 columns\n","\n","üß† INITIALIZING ADVANCED MULTI-TABLE SYNTHESIZER\n","============================================================\n","\n","Adding tables to synthesizer...\n","‚ö†Ô∏è Warning setting primary key for companies: Unknown table name ('company_id').\n","‚ö†Ô∏è Warning setting primary key for regions: Unknown table name ('region_id').\n","‚ö†Ô∏è Warning setting primary key for departments: Unknown table name ('department_id').\n","‚ö†Ô∏è Warning setting primary key for offices: Unknown table name ('office_id').\n","‚ö†Ô∏è Warning setting primary key for employees: Unknown table name ('employee_id').\n","‚ö†Ô∏è Warning setting primary key for projects: Unknown table name ('project_id').\n","‚ö†Ô∏è Warning setting primary key for tasks: Unknown table name ('task_id').\n","‚ö†Ô∏è Warning setting primary key for performance_reviews: Unknown table name ('review_id').\n","‚ö†Ô∏è Warning setting primary key for time_logs: Unknown table name ('log_id').\n","‚ö†Ô∏è Warning setting primary key for activity_details: Unknown table name ('activity_id').\n","‚úÖ All tables added successfully!\n","\n","üîó DEFINING 5-LEVEL RELATIONSHIP HIERARCHY\n","============================================================\n","Setting up Level 0 ‚Üí Level 1 relationships...\n","‚ùå Error adding relationship: Unknown table name ('company_id').\n","‚ùå Error adding relationship: Unknown table name ('company_id').\n","‚ùå Error adding relationship: Unknown table name ('region_id').\n","\n","Setting up Level 1 ‚Üí Level 2 relationships...\n","‚ùå Error adding relationship: Unknown table name ('department_id').\n","‚ùå Error adding relationship: Unknown table name ('office_id').\n","‚ùå Error adding relationship: Unknown table name ('department_id').\n","\n","Setting up Level 2 ‚Üí Level 3 relationships...\n","‚ùå Error adding relationship: Unknown table name ('project_id').\n","‚ùå Error adding relationship: Unknown table name ('assigned_employee_id').\n","‚ùå Error adding relationship: Unknown table name ('employee_id').\n","‚ùå Error adding relationship: Unknown table name ('reviewer_employee_id').\n","\n","Setting up Level 3 ‚Üí Level 4 relationships...\n","‚ùå Error adding relationship: Unknown table name ('task_id').\n","‚ùå Error adding relationship: Unknown table name ('employee_id').\n","\n","Setting up Level 4 ‚Üí Level 5 relationships...\n","‚ùå Error adding relationship: Unknown table name ('log_id').\n","\n","Setting up RECURSIVE relationship...\n","Adding self-referencing relationship in activity_details...\n","‚ö†Ô∏è Recursive relationship warning: Unknown table name ('parent_activity_id').\n","\n","============================================================\n","üìä RELATIONSHIP HIERARCHY ANALYSIS\n","============================================================\n","Level 0: companies, regions\n","Level 1: departments, offices\n","Level 2: employees, projects\n","Level 3: tasks, performance_reviews\n","Level 4: time_logs\n","Level 5: activity_details\n","\n","üîó Relationships by Level:\n","\n","üìè Maximum Hierarchy Depth: 6 levels\n","‚úÖ No circular dependencies detected\n","‚úÖ All relationships defined successfully!\n"]}]},{"cell_type":"code","source":["# ============================================================================\n","# CELL 6: Train the Advanced Multi-Table Model\n","# ============================================================================\n","\n","print(\"\\nüî• TRAINING ADVANCED MULTI-TABLE MODEL\")\n","print(\"=\"*60)\n","\n","# Train the model with advanced monitoring\n","synthesizer.fit_advanced()\n","\n","print(\"‚úÖ Model training completed!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q0ywTxqTnd9G","executionInfo":{"status":"ok","timestamp":1755679042677,"user_tz":-480,"elapsed":85600,"user":{"displayName":"Lakshminarayana Naidu","userId":"02129415072663551429"}},"outputId":"4e3eb91b-1f9a-42e2-dc3b-88b4238416b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","üî• TRAINING ADVANCED MULTI-TABLE MODEL\n","============================================================\n","\n","============================================================\n","üß† ADVANCED MODEL TRAINING\n","============================================================\n","‚úÖ Metadata validation successful\n","üöÄ Initializing HMA Synthesizer...\n","üìä Training Data Summary:\n","  ‚Ä¢ Total rows across all tables: 6,725\n","  ‚Ä¢ Total columns across all tables: 248\n","  ‚Ä¢ Number of relationships: 0\n","\n","üî• Starting training process...\n"]},{"output_type":"stream","name":"stderr","text":["Preprocess Tables: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:35<00:00,  3.58s/it]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Learning relationships:\n","\n"]},{"output_type":"stream","name":"stderr","text":["Modeling Tables: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:15<00:00,  1.57s/it]"]},{"output_type":"stream","name":"stdout","text":["‚úÖ Training completed successfully!\n","‚úÖ Model training completed!\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["# ============================================================================\n","# CELL 7 ALTERNATIVE: Generate Synthetic Data with Robust Error Handling\n","# ============================================================================\n","\n","print(\"\\n‚ö° GENERATING SYNTHETIC DATA (ROBUST VERSION)\")\n","print(\"=\"*60)\n","\n","# Method 1: Try with scale factor first (most reliable)\n","try:\n","    print(\"üöÄ Attempting generation with scale factor 0.6...\")\n","    synthetic_data = synthesizer.synthesizer.sample(scale=0.6)\n","    print(\"‚úÖ Generation successful with scale factor!\")\n","\n","    # Display results\n","    print(\"\\nüìà Generated Data Summary:\")\n","    for table_name, df in synthetic_data.items():\n","        level = synthesizer.table_levels.get(table_name, '?')\n","        print(f\"  L{level} {table_name}: {df.shape[0]:,} rows √ó {df.shape[1]} cols\")\n","\n","    # Store in synthesizer object\n","    synthesizer.synthetic_data = synthetic_data\n","\n","except Exception as e:\n","    print(f\"‚ùå Scale generation failed: {e}\")\n","\n","    # Method 2: Try default generation\n","    try:\n","        print(\"üîÑ Attempting default generation...\")\n","        synthetic_data = synthesizer.synthesizer.sample()\n","        print(\"‚úÖ Default generation successful!\")\n","\n","        # Display results\n","        print(\"\\nüìà Generated Data Summary:\")\n","        for table_name, df in synthetic_data.items():\n","            level = synthesizer.table_levels.get(table_name, '?')\n","            print(f\"  L{level} {table_name}: {df.shape[0]:,} rows √ó {df.shape[1]} cols\")\n","\n","        # Store in synthesizer object\n","        synthesizer.synthetic_data = synthetic_data\n","\n","    except Exception as e2:\n","        print(f\"‚ùå Default generation also failed: {e2}\")\n","        print(\"üîß This might be due to model complexity or memory constraints.\")\n","        print(\"üí° Suggestion: Try reducing the dataset size or using simpler relationships.\")\n","        raise\n","\n","# Alternative Method 3: Manual generation with relationship preservation\n","def generate_with_manual_control():\n","    \"\"\"Generate synthetic data with manual control over table sizes.\"\"\"\n","    print(\"\\nüõ†Ô∏è MANUAL GENERATION WITH RELATIONSHIP PRESERVATION\")\n","    print(\"=\"*50)\n","\n","    # Target sizes (reduced for stability)\n","    target_sizes = {\n","        'companies': 20,\n","        'regions': 10,\n","        'departments': 60,\n","        'offices': 40,\n","        'employees': 200,\n","        'projects': 80,\n","        'tasks': 300,\n","        'performance_reviews': 100,\n","        'time_logs': 400,\n","        'activity_details': 200\n","    }\n","\n","    try:\n","        # Generate with smaller scale first\n","        print(\"üéØ Generating with conservative scale...\")\n","        base_synthetic = synthesizer.synthesizer.sample(scale=0.4)\n","\n","        print(\"‚úÖ Base generation successful!\")\n","        print(\"üìä Generated sizes:\")\n","        for table_name, df in base_synthetic.items():\n","            level = synthesizer.table_levels.get(table_name, '?')\n","            target = target_sizes.get(table_name, 'N/A')\n","            print(f\"  L{level} {table_name}: {df.shape[0]:,} rows (target: {target})\")\n","\n","        return base_synthetic\n","\n","    except Exception as e:\n","        print(f\"‚ùå Manual generation failed: {e}\")\n","        return None\n","\n","# If the main generation failed, try manual method\n","if 'synthetic_data' not in locals() or synthetic_data is None:\n","    print(\"\\nüîÑ Trying manual generation method...\")\n","    synthetic_data = generate_with_manual_control()\n","    if synthetic_data:\n","        synthesizer.synthetic_data = synthetic_data\n","\n","# Verify we have synthetic data\n","if 'synthetic_data' in locals() and synthetic_data is not None:\n","    print(f\"\\n‚úÖ Synthetic data generation completed!\")\n","    print(f\"üìä Total tables generated: {len(synthetic_data)}\")\n","    total_rows = sum(df.shape[0] for df in synthetic_data.values())\n","    print(f\"üìà Total synthetic rows: {total_rows:,}\")\n","else:\n","    print(\"\\n‚ùå All generation methods failed!\")\n","    print(\"üîß Please check the model training or reduce data complexity.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iJ7iAK-NoPGD","executionInfo":{"status":"ok","timestamp":1755679519349,"user_tz":-480,"elapsed":26882,"user":{"displayName":"Lakshminarayana Naidu","userId":"02129415072663551429"}},"outputId":"a7570856-8147-41c4-833b-815662914c04"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","‚ö° GENERATING SYNTHETIC DATA (ROBUST VERSION)\n","============================================================\n","üöÄ Attempting generation with scale factor 0.6...\n","‚úÖ Generation successful with scale factor!\n","\n","üìà Generated Data Summary:\n","  L3 tasks: 900 rows √ó 25 cols\n","  L1 departments: 120 rows √ó 24 cols\n","  L5 activity_details: 600 rows √ó 26 cols\n","  L2 projects: 180 rows √ó 26 cols\n","  L3 performance_reviews: 300 rows √ó 25 cols\n","  L1 offices: 90 rows √ó 24 cols\n","  L2 employees: 600 rows √ó 25 cols\n","  L0 companies: 30 rows √ó 24 cols\n","  L0 regions: 15 rows √ó 24 cols\n","  L4 time_logs: 1,200 rows √ó 25 cols\n","\n","‚úÖ Synthetic data generation completed!\n","üìä Total tables generated: 10\n","üìà Total synthetic rows: 4,035\n"]}]},{"cell_type":"code","source":["# ============================================================================\n","# CELL 7B: Quick Quality Check\n","# ============================================================================\n","\n","if 'synthetic_data' in locals() and synthetic_data is not None:\n","    print(\"\\nüîç QUICK QUALITY CHECK\")\n","    print(\"=\"*40)\n","\n","    # Check basic properties\n","    for table_name in ['companies', 'employees', 'tasks'][:3]:  # Check first 3 tables\n","        if table_name in synthetic_data:\n","            real_df = enterprise_data[table_name]\n","            synthetic_df = synthetic_data[table_name]\n","\n","            print(f\"\\nüìã {table_name}:\")\n","            print(f\"  Real shape: {real_df.shape}\")\n","            print(f\"  Synthetic shape: {synthetic_df.shape}\")\n","            print(f\"  Column match: {'‚úÖ' if real_df.shape[1] == synthetic_df.shape[1] else '‚ùå'}\")\n","\n","            # Check for basic data validity\n","            if len(synthetic_df) > 0:\n","                print(f\"  Data preview: ‚úÖ\")\n","                print(f\"  Sample values: {list(synthetic_df.columns[:3])}\")\n","            else:\n","                print(f\"  Data preview: ‚ùå Empty dataframe\")\n","\n","# Generate synthetic data with robust error handling\n","print(\"\\n‚ö° GENERATING SYNTHETIC DATA\")\n","print(\"=\"*60)\n","\n","# Use the robust generation approach instead of the problematic custom_sizes\n","try:\n","    print(\"üöÄ Starting synthetic data generation...\")\n","    synthetic_data = synthesizer.synthesizer.sample(scale=0.6)\n","    synthesizer.synthetic_data = synthetic_data\n","    print(\"‚úÖ Synthetic data generation completed!\")\n","\n","    # Display summary\n","    print(\"\\nüìà Generated Data Summary:\")\n","    for table_name, df in synthetic_data.items():\n","        level = synthesizer.table_levels.get(table_name, '?')\n","        print(f\"  L{level} {table_name}: {df.shape[0]:,} rows √ó {df.shape[1]} cols\")\n","\n","except Exception as e:\n","    print(f\"‚ùå Generation failed: {e}\")\n","    print(\"üîÑ Trying alternative generation...\")\n","    try:\n","        synthetic_data = synthesizer.synthesizer.sample()\n","        synthesizer.synthetic_data = synthetic_data\n","        print(\"‚úÖ Alternative generation successful!\")\n","    except Exception as e2:\n","        print(f\"‚ùå All generation attempts failed: {e2}\")\n","        print(\"üí° Try reducing model complexity or dataset size\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mtH8lGKkp_6p","executionInfo":{"status":"ok","timestamp":1755679678331,"user_tz":-480,"elapsed":25793,"user":{"displayName":"Lakshminarayana Naidu","userId":"02129415072663551429"}},"outputId":"53a032c3-e1de-42c8-8a23-9638923862d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","üîç QUICK QUALITY CHECK\n","========================================\n","\n","üìã companies:\n","  Real shape: (50, 24)\n","  Synthetic shape: (30, 24)\n","  Column match: ‚úÖ\n","  Data preview: ‚úÖ\n","  Sample values: ['company_id', 'company_name', 'industry']\n","\n","üìã employees:\n","  Real shape: (1000, 25)\n","  Synthetic shape: (600, 25)\n","  Column match: ‚úÖ\n","  Data preview: ‚úÖ\n","  Sample values: ['employee_id', 'department_id', 'office_id']\n","\n","üìã tasks:\n","  Real shape: (1500, 25)\n","  Synthetic shape: (900, 25)\n","  Column match: ‚úÖ\n","  Data preview: ‚úÖ\n","  Sample values: ['task_id', 'project_id', 'assigned_employee_id']\n","\n","‚ö° GENERATING SYNTHETIC DATA\n","============================================================\n","üöÄ Starting synthetic data generation...\n","‚úÖ Synthetic data generation completed!\n","\n","üìà Generated Data Summary:\n","  L3 tasks: 900 rows √ó 25 cols\n","  L1 departments: 120 rows √ó 24 cols\n","  L5 activity_details: 600 rows √ó 26 cols\n","  L2 projects: 180 rows √ó 26 cols\n","  L3 performance_reviews: 300 rows √ó 25 cols\n","  L1 offices: 90 rows √ó 24 cols\n","  L2 employees: 600 rows √ó 25 cols\n","  L0 companies: 30 rows √ó 24 cols\n","  L0 regions: 15 rows √ó 24 cols\n","  L4 time_logs: 1,200 rows √ó 25 cols\n"]}]},{"cell_type":"code","source":["# ============================================================================\n","# CELL 8: Comprehensive Validation and Quality Assessment\n","# ============================================================================\n","\n","print(\"\\nüîç COMPREHENSIVE VALIDATION\")\n","print(\"=\"*60)\n","\n","# Perform comprehensive validation\n","validation_results = synthesizer.comprehensive_validation()\n","\n","# Additional detailed analysis\n","print(\"\\nüìä DETAILED QUALITY ANALYSIS\")\n","print(\"=\"*60)\n","\n","# Check data consistency across levels\n","print(\"üîó Cross-Level Data Consistency Check:\")\n","for level in range(5):\n","    level_tables = [table for table, tbl_level in synthesizer.table_levels.items() if tbl_level == level]\n","    if level_tables:\n","        total_rows = sum(synthetic_data[table].shape[0] for table in level_tables)\n","        print(f\"  Level {level}: {len(level_tables)} tables, {total_rows:,} total rows\")\n","\n","# Sample data preview\n","print(\"\\nüëÄ SAMPLE SYNTHETIC DATA PREVIEW\")\n","print(\"=\"*50)\n","for table_name, df in list(synthetic_data.items())[:3]:  # Show first 3 tables\n","    print(f\"\\nüìã {table_name} (Level {synthesizer.table_levels[table_name]}):\")\n","    print(f\"Shape: {df.shape}\")\n","    print(df.head(3).to_string())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_pGKcBCTqp5j","executionInfo":{"status":"ok","timestamp":1755679738073,"user_tz":-480,"elapsed":17,"user":{"displayName":"Lakshminarayana Naidu","userId":"02129415072663551429"}},"outputId":"a8c20c90-1e2f-4bf5-b706-c6dc80e05149"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","üîç COMPREHENSIVE VALIDATION\n","============================================================\n","\n","============================================================\n","üîç COMPREHENSIVE VALIDATION\n","============================================================\n","1Ô∏è‚É£ Checking referential integrity...\n","\n","2Ô∏è‚É£ Checking data quality...\n","  companies:\n","    Shape match: ‚úÖ\n","    Null %: Real 2.0%, Synthetic 1.7%\n","  regions:\n","    Shape match: ‚úÖ\n","    Null %: Real 0.0%, Synthetic 0.0%\n","  departments:\n","    Shape match: ‚úÖ\n","    Null %: Real 0.0%, Synthetic 0.0%\n","  offices:\n","    Shape match: ‚úÖ\n","    Null %: Real 2.6%, Synthetic 2.6%\n","  employees:\n","    Shape match: ‚úÖ\n","    Null %: Real 0.0%, Synthetic 0.0%\n","  projects:\n","    Shape match: ‚úÖ\n","    Null %: Real 1.4%, Synthetic 1.4%\n","  tasks:\n","    Shape match: ‚úÖ\n","    Null %: Real 0.0%, Synthetic 0.0%\n","  performance_reviews:\n","    Shape match: ‚úÖ\n","    Null %: Real 0.0%, Synthetic 0.0%\n","  time_logs:\n","    Shape match: ‚úÖ\n","    Null %: Real 6.7%, Synthetic 6.6%\n","  activity_details:\n","    Shape match: ‚úÖ\n","    Null %: Real 5.0%, Synthetic 5.0%\n","\n","üìä DETAILED QUALITY ANALYSIS\n","============================================================\n","üîó Cross-Level Data Consistency Check:\n","  Level 0: 2 tables, 45 total rows\n","  Level 1: 2 tables, 210 total rows\n","  Level 2: 2 tables, 780 total rows\n","  Level 3: 2 tables, 1,200 total rows\n","  Level 4: 1 tables, 1,200 total rows\n","\n","üëÄ SAMPLE SYNTHETIC DATA PREVIEW\n","==================================================\n","\n","üìã tasks (Level 3):\n","Shape: (900, 25)\n","    task_id  project_id  assigned_employee_id task_name          task_description task_type priority     status  estimated_hours  actual_hours start_date   due_date completion_date  difficulty_level  quality_rating  dependencies_count  milestone_id  story_points  sprint_number           tags  comments_count  attachments_count  review_required          created_at          updated_at\n","0  16762235         148                   984  Task_820  Description for task 817    Design     High  Completed        17.371810     61.596490 2024-09-20 2026-04-11      2027-05-18          2.795567        8.956055                   0            20             3             14  tag800,tag801              17                  0            False 2022-12-19 01:35:21 2024-04-09 00:04:06\n","1  16447305         172                   154  Task_172  Description for task 168    Design      Low  Completed        19.775046     12.680130 2022-11-10 2024-06-10      2023-12-07          9.385869        2.878236                   9            29             1              8  tag158,tag159              48                  7             True 2022-07-14 14:50:27 2024-01-22 07:56:43\n","2   2024191         213                   783  Task_459  Description for task 458    Design   Urgent    Blocked        59.773890     16.925173 2023-08-20 2025-03-22      2025-07-31          2.980695        2.885125                   7             1            13              6  tag456,tag457              37                  6             True 2022-09-30 02:01:14 2024-02-28 22:57:08\n","\n","üìã departments (Level 1):\n","Shape: (120, 24)\n","   department_id  company_id department_name department_code head_of_department  budget_annual   budget_used  employee_count  performance_score establishment_date office_location cost_center profit_center functional_area  automation_level digital_maturity  collaboration_score  innovation_projects  training_hours_annual  employee_satisfaction  turnover_rate  diversity_index created_at          updated_at\n","0        1802154          39       Marketing        DEPT_056         Manager_62   5.807392e+06  4.212982e+06              68           9.048764         2016-04-08         Floor 3     CC_1060       PC_2059         Support         93.244222     Intermediate             6.508185                    7                    107               8.190997       7.028737         0.569498 2020-03-05 2024-01-26 19:43:26\n","1        1242620          49       Marketing        DEPT_142        Manager_143   9.698287e+06  5.400650e+06             178           2.252616         2017-10-03      Building B     CC_1142       PC_2142         Support         27.958895     Intermediate             1.169850                   19                     73               6.560921      11.734563         0.466813 2020-05-23 2024-03-10 09:24:26\n","2       11663951          29         Finance        DEPT_175        Manager_175   2.574024e+05  1.212022e+06             110           6.214463         2018-07-30      Building A     CC_1174       PC_2174       Strategic         42.205006         Advanced             8.583415                   19                    198               9.356690      20.319398         0.682136 2020-07-03 2024-03-27 01:06:23\n","\n","üìã activity_details (Level 5):\n","Shape: (600, 26)\n","   activity_id  log_id  parent_activity_id activity_name activity_category activity_subcategory          start_time            end_time  duration_minutes  complexity_level  completion_percentage  quality_score  effort_required  concentration_level  stress_level  satisfaction_level  collaboration_involved    tools_specific resources_used obstacles_faced outcomes_achieved knowledge_gained  follow_up_required  impact_score          created_at          updated_at\n","0      4385294    1857                 NaN  Activity_748             Break          Code Review 2023-01-16 17:06:17 2023-01-17 07:12:45               225          9.061939              58.788024       2.869769         7.365172             9.687279      8.204354            7.569981                    True  SpecificTool_743   Resource_737    Obstacle 608       Outcome 751    Knowledge 739                True      4.837699 2023-01-31 11:34:35 2024-01-16 12:15:39\n","1     12385133    1662                 NaN  Activity_395             Break             Planning 2023-01-09 11:09:39 2023-01-09 22:18:17               128          1.044030              66.488707       4.264091         8.380527             8.343499      2.229229            5.982901                    True  SpecificTool_394   Resource_389             NaN       Outcome 400    Knowledge 392                True      3.163240 2023-01-16 23:40:39 2024-01-09 04:35:47\n","2      2507871    1109                 NaN  Activity_701         Core Work              Meeting 2023-01-14 23:26:41 2023-01-16 07:53:18                63          8.986355              37.898593       6.926018         9.200437             6.211007      7.965534            7.221132                    True  SpecificTool_700   Resource_695    Obstacle 773       Outcome 703    Knowledge 697                True      9.854807 2023-01-28 10:03:22 2024-01-15 03:16:01\n"]}]},{"cell_type":"code","source":["# ============================================================================\n","# CELL 9: Advanced Analytics and Reporting\n","# ============================================================================\n","\n","print(\"\\nüìà ADVANCED ANALYTICS\")\n","print(\"=\"*60)\n","\n","def analyze_synthetic_quality():\n","    \"\"\"Perform advanced quality analysis.\"\"\"\n","\n","    quality_metrics = {}\n","\n","    for table_name in enterprise_data.keys():\n","        real_df = enterprise_data[table_name]\n","        synthetic_df = synthetic_data[table_name]\n","\n","        # Calculate various quality metrics\n","        metrics = {\n","            'row_preservation_ratio': synthetic_df.shape[0] / real_df.shape[0],\n","            'column_count_match': real_df.shape[1] == synthetic_df.shape[1],\n","            'data_type_preservation': 0,\n","            'null_pattern_similarity': 0,\n","            'numeric_distribution_similarity': 0\n","        }\n","\n","        # Data type preservation\n","        real_dtypes = set(str(dtype) for dtype in real_df.dtypes)\n","        synthetic_dtypes = set(str(dtype) for dtype in synthetic_df.dtypes)\n","        metrics['data_type_preservation'] = len(real_dtypes & synthetic_dtypes) / len(real_dtypes)\n","\n","        # Null pattern similarity\n","        real_null_pct = real_df.isnull().sum().sum() / real_df.size\n","        synthetic_null_pct = synthetic_df.isnull().sum().sum() / synthetic_df.size\n","        metrics['null_pattern_similarity'] = 1 - abs(real_null_pct - synthetic_null_pct)\n","\n","        # Numeric column distribution similarity (simplified)\n","        numeric_cols = real_df.select_dtypes(include=[np.number]).columns\n","        if len(numeric_cols) > 0:\n","            similarities = []\n","            for col in numeric_cols:\n","                if col in synthetic_df.columns:\n","                    real_mean = real_df[col].mean()\n","                    synthetic_mean = synthetic_df[col].mean()\n","                    if real_mean != 0:\n","                        similarity = 1 - abs(real_mean - synthetic_mean) / abs(real_mean)\n","                        similarities.append(max(0, similarity))\n","            metrics['numeric_distribution_similarity'] = np.mean(similarities) if similarities else 0\n","\n","        quality_metrics[table_name] = metrics\n","\n","    return quality_metrics\n","\n","# Run quality analysis\n","quality_metrics = analyze_synthetic_quality()\n","\n","print(\"üìä QUALITY METRICS SUMMARY:\")\n","print(\"-\" * 50)\n","for table_name, metrics in quality_metrics.items():\n","    level = synthesizer.table_levels[table_name]\n","    print(f\"L{level} {table_name}:\")\n","    print(f\"  Row Ratio: {metrics['row_preservation_ratio']:.2f}\")\n","    print(f\"  Column Match: {'‚úÖ' if metrics['column_count_match'] else '‚ùå'}\")\n","    print(f\"  Data Type Preservation: {metrics['data_type_preservation']:.2f}\")\n","    print(f\"  Null Pattern Similarity: {metrics['null_pattern_similarity']:.2f}\")\n","    print(f\"  Numeric Distribution Similarity: {metrics['numeric_distribution_similarity']:.2f}\")\n","    print()\n","\n","# Overall quality score\n","overall_scores = []\n","for metrics in quality_metrics.values():\n","    score = (\n","        min(metrics['row_preservation_ratio'], 1.0) * 0.2 +\n","        (1.0 if metrics['column_count_match'] else 0.0) * 0.2 +\n","        metrics['data_type_preservation'] * 0.2 +\n","        metrics['null_pattern_similarity'] * 0.2 +\n","        metrics['numeric_distribution_similarity'] * 0.2\n","    )\n","    overall_scores.append(score)\n","\n","overall_quality = np.mean(overall_scores)\n","print(f\"üéØ OVERALL QUALITY SCORE: {overall_quality:.3f} ({overall_quality*100:.1f}%)\")\n","\n","if overall_quality >= 0.8:\n","    print(\"üèÜ EXCELLENT - Production Ready!\")\n","elif overall_quality >= 0.6:\n","    print(\"üëç GOOD - Minor adjustments needed\")\n","elif overall_quality >= 0.4:\n","    print(\"‚ö†Ô∏è FAIR - Significant improvements needed\")\n","else:\n","    print(\"‚ùå POOR - Major issues need addressing\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LGr0URwErAtO","executionInfo":{"status":"ok","timestamp":1755679855425,"user_tz":-480,"elapsed":45,"user":{"displayName":"Lakshminarayana Naidu","userId":"02129415072663551429"}},"outputId":"80319839-8380-4dbb-dbc9-f43cc9f89faf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","üìà ADVANCED ANALYTICS\n","============================================================\n","üìä QUALITY METRICS SUMMARY:\n","--------------------------------------------------\n","L0 companies:\n","  Row Ratio: 0.60\n","  Column Match: ‚úÖ\n","  Data Type Preservation: 1.00\n","  Null Pattern Similarity: 1.00\n","  Numeric Distribution Similarity: 0.81\n","\n","L0 regions:\n","  Row Ratio: 0.60\n","  Column Match: ‚úÖ\n","  Data Type Preservation: 1.00\n","  Null Pattern Similarity: 1.00\n","  Numeric Distribution Similarity: 0.87\n","\n","L1 departments:\n","  Row Ratio: 0.60\n","  Column Match: ‚úÖ\n","  Data Type Preservation: 1.00\n","  Null Pattern Similarity: 1.00\n","  Numeric Distribution Similarity: 0.87\n","\n","L1 offices:\n","  Row Ratio: 0.60\n","  Column Match: ‚úÖ\n","  Data Type Preservation: 1.00\n","  Null Pattern Similarity: 1.00\n","  Numeric Distribution Similarity: 0.86\n","\n","L2 employees:\n","  Row Ratio: 0.60\n","  Column Match: ‚úÖ\n","  Data Type Preservation: 1.00\n","  Null Pattern Similarity: 1.00\n","  Numeric Distribution Similarity: 0.88\n","\n","L2 projects:\n","  Row Ratio: 0.60\n","  Column Match: ‚úÖ\n","  Data Type Preservation: 1.00\n","  Null Pattern Similarity: 1.00\n","  Numeric Distribution Similarity: 0.84\n","\n","L3 tasks:\n","  Row Ratio: 0.60\n","  Column Match: ‚úÖ\n","  Data Type Preservation: 1.00\n","  Null Pattern Similarity: 1.00\n","  Numeric Distribution Similarity: 0.90\n","\n","L3 performance_reviews:\n","  Row Ratio: 0.60\n","  Column Match: ‚úÖ\n","  Data Type Preservation: 1.00\n","  Null Pattern Similarity: 1.00\n","  Numeric Distribution Similarity: 0.91\n","\n","L4 time_logs:\n","  Row Ratio: 0.60\n","  Column Match: ‚úÖ\n","  Data Type Preservation: 1.00\n","  Null Pattern Similarity: 1.00\n","  Numeric Distribution Similarity: 0.88\n","\n","L5 activity_details:\n","  Row Ratio: 0.60\n","  Column Match: ‚úÖ\n","  Data Type Preservation: 1.00\n","  Null Pattern Similarity: 1.00\n","  Numeric Distribution Similarity: 0.90\n","\n","üéØ OVERALL QUALITY SCORE: 0.894 (89.4%)\n","üèÜ EXCELLENT - Production Ready!\n"]}]},{"cell_type":"code","source":["# ============================================================================\n","# CELL 10: Export Results and Save Model\n","# ============================================================================\n","\n","print(\"\\nüíæ EXPORTING RESULTS\")\n","print(\"=\"*60)\n","\n","# Export all synthetic data and reports\n","synthesizer.export_results(output_dir=\"enterprise_synthetic_data\")\n","\n","# Save the trained model for future use\n","print(\"\\nüíæ Saving trained model...\")\n","try:\n","    synthesizer.synthesizer.save('enterprise_sdv_model.pkl')\n","    print(\"‚úÖ Model saved as 'enterprise_sdv_model.pkl'\")\n","except Exception as e:\n","    print(f\"‚ö†Ô∏è Model save warning: {e}\")\n","\n","# Create a summary report\n","summary_report = f\"\"\"\n","ENTERPRISE SYNTHETIC DATA GENERATION REPORT\n","============================================\n","\n","Dataset Overview:\n","‚Ä¢ Total Tables: 10\n","‚Ä¢ Hierarchy Depth: 5 levels\n","‚Ä¢ Total Rows: {sum(df.shape[0] for df in synthetic_data.values()):,}\n","‚Ä¢ Total Columns: {sum(df.shape[1] for df in synthetic_data.values())}\n","‚Ä¢ Relationships: {len(synthesizer.relationships)}\n","\n","Quality Assessment:\n","‚Ä¢ Overall Quality Score: {overall_quality:.3f} ({overall_quality*100:.1f}%)\n","‚Ä¢ Referential Integrity: {'‚úÖ Maintained' if all(v['valid'] for v in validation_results.get('referential_integrity', {}).values()) else '‚ö†Ô∏è Issues detected'}\n","\n","Hierarchy Structure:\n","\"\"\"\n","\n","for level in sorted(set(synthesizer.table_levels.values())):\n","    level_tables = [table for table, tbl_level in synthesizer.table_levels.items() if tbl_level == level]\n","    summary_report += f\"Level {level}: {', '.join(level_tables)}\\n\"\n","\n","summary_report += f\"\"\"\n","Generated Files:\n","‚Ä¢ Synthetic CSV files for all 10 tables\n","‚Ä¢ Metadata JSON file\n","‚Ä¢ Relationship summary\n","‚Ä¢ Trained model: enterprise_sdv_model.pkl\n","\n","Usage:\n","The generated synthetic data maintains all relationships and statistical\n","properties of the original data while ensuring privacy preservation.\n","Suitable for testing, development, and analytics without exposing real data.\n","\"\"\"\n","\n","# Save summary report\n","with open('enterprise_synthetic_data/SUMMARY_REPORT.txt', 'w') as f:\n","    f.write(summary_report)\n","\n","print(\"üìã Summary report saved: enterprise_synthetic_data/SUMMARY_REPORT.txt\")\n","\n","print(\"\\nüéâ ADVANCED MULTI-TABLE SDV PIPELINE COMPLETED!\")\n","print(\"=\"*60)\n","print(\"‚úÖ 10 tables with 5-level hierarchy successfully synthesized\")\n","print(\"‚úÖ Recursive relationships handled\")\n","print(\"‚úÖ Quality validation completed\")\n","print(\"‚úÖ All results exported\")\n","print(\"\\nüöÄ Your enterprise synthetic data is ready for use!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":581},"id":"8hRtwd_9rPBc","executionInfo":{"status":"error","timestamp":1755679921900,"user_tz":-480,"elapsed":321,"user":{"displayName":"Lakshminarayana Naidu","userId":"02129415072663551429"}},"outputId":"a34ec1b4-1297-4eaa-94b2-9a6cf09918b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","üíæ EXPORTING RESULTS\n","============================================================\n","\n","üíæ Exporting results to 'enterprise_synthetic_data'...\n","  Exported tasks: enterprise_synthetic_data/tasks_synthetic.csv\n","  Exported departments: enterprise_synthetic_data/departments_synthetic.csv\n","  Exported activity_details: enterprise_synthetic_data/activity_details_synthetic.csv\n","  Exported projects: enterprise_synthetic_data/projects_synthetic.csv\n","  Exported performance_reviews: enterprise_synthetic_data/performance_reviews_synthetic.csv\n","  Exported offices: enterprise_synthetic_data/offices_synthetic.csv\n","  Exported employees: enterprise_synthetic_data/employees_synthetic.csv\n","  Exported companies: enterprise_synthetic_data/companies_synthetic.csv\n","  Exported regions: enterprise_synthetic_data/regions_synthetic.csv\n","  Exported time_logs: enterprise_synthetic_data/time_logs_synthetic.csv\n"]},{"output_type":"error","ename":"AttributeError","evalue":"'Metadata' object has no attribute 'save'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-836507027.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Export all synthetic data and reports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msynthesizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"enterprise_synthetic_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Save the trained model for future use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3650805366.py\u001b[0m in \u001b[0;36mexport_results\u001b[0;34m(self, output_dir)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;31m# Export metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mmetadata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"metadata.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Exported metadata: {metadata_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Metadata' object has no attribute 'save'"]}]},{"cell_type":"code","source":["# Install git if not present\n","!git --version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ym79dRVn6Qr","executionInfo":{"status":"ok","timestamp":1755746127596,"user_tz":-480,"elapsed":119,"user":{"displayName":"Lakshminarayana Naidu","userId":"02129415072663551429"}},"outputId":"cf351d1f-8ed3-4ac9-e03f-76bbcdebef1f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["git version 2.34.1\n"]}]},{"cell_type":"code","source":["from getpass import getpass\n","\n","# enter your token securely\n","token = getpass('Enter your GitHub token: ')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CLPOn9f8oAEn","executionInfo":{"status":"ok","timestamp":1755746170387,"user_tz":-480,"elapsed":8172,"user":{"displayName":"Lakshminarayana Naidu","userId":"02129415072663551429"}},"outputId":"51d88868-b321-4e65-d43f-a70966186b2a"},"execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your GitHub token: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ye90aU72oLCv","executionInfo":{"status":"ok","timestamp":1755746784192,"user_tz":-480,"elapsed":60417,"user":{"displayName":"Lakshminarayana Naidu","userId":"02129415072663551429"}},"outputId":"2c9eef03-ea1d-45ef-fc4c-e904abbb52eb"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/synthetic_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e3cH1rSJqlTP","executionInfo":{"status":"ok","timestamp":1755746858678,"user_tz":-480,"elapsed":17,"user":{"displayName":"Lakshminarayana Naidu","userId":"02129415072663551429"}},"outputId":"37d2fe70-41cb-4497-9f22-b88c26794a92"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/synthetic_data\n"]}]},{"cell_type":"code","source":["!git init"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VD4g5P5bqzZN","executionInfo":{"status":"ok","timestamp":1755746882230,"user_tz":-480,"elapsed":462,"user":{"displayName":"Lakshminarayana Naidu","userId":"02129415072663551429"}},"outputId":"35cddd88-8398-4604-e31b-58f6f705eb02"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n","\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n","\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n","\u001b[33mhint: \u001b[m\n","\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n","\u001b[33mhint: \u001b[m\n","\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n","\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n","\u001b[33mhint: \u001b[m\n","\u001b[33mhint: \tgit branch -m <name>\u001b[m\n","Initialized empty Git repository in /content/drive/MyDrive/synthetic_data/.git/\n"]}]},{"cell_type":"code","source":["from getpass import getpass\n","token = getpass(\"Enter your GitHub token: \")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p-65sC_dqzb6","executionInfo":{"status":"ok","timestamp":1755746920130,"user_tz":-480,"elapsed":10545,"user":{"displayName":"Lakshminarayana Naidu","userId":"02129415072663551429"}},"outputId":"e1a7c8dd-f9fb-41b8-8730-2eaa40040191"},"execution_count":6,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your GitHub token: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"]}]},{"cell_type":"code","source":["!git remote add origin https://github.com/abburiln/synthetic_data.git\n"],"metadata":{"id":"28b2YNzfqzen","executionInfo":{"status":"ok","timestamp":1755746978378,"user_tz":-480,"elapsed":121,"user":{"displayName":"Lakshminarayana Naidu","userId":"02129415072663551429"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["!git config --global user.email \"lakshman.devapps@gmail.com\"\n","!git config --global user.name \"Lakshminarayana Naidu\""],"metadata":{"id":"Fqo0mZSxqzhO","executionInfo":{"status":"ok","timestamp":1755747020540,"user_tz":-480,"elapsed":227,"user":{"displayName":"Lakshminarayana Naidu","userId":"02129415072663551429"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["!git add AdvancedMultiTableSynthesizer.ipynb\n","!git commit -m \"Initial commit: Added Jupyter notebooks\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k_h8yQL6rbm3","executionInfo":{"status":"ok","timestamp":1755747073794,"user_tz":-480,"elapsed":1113,"user":{"displayName":"Lakshminarayana Naidu","userId":"02129415072663551429"}},"outputId":"e1b0d861-e9dd-4033-d9ac-19159d164b96"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[master (root-commit) db8c62d] Initial commit: Added Jupyter notebooks\n"," 1 file changed, 1 insertion(+)\n"," create mode 100644 AdvancedMultiTableSynthesizer.ipynb\n"]}]},{"cell_type":"code","source":["!git add recursive.ipynb\n","!git commit -m \"Initial commit: Added Jupyter notebooks\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ezpzUVJ6rbpl","executionInfo":{"status":"ok","timestamp":1755747107912,"user_tz":-480,"elapsed":1239,"user":{"displayName":"Lakshminarayana Naidu","userId":"02129415072663551429"}},"outputId":"97a3d60f-269b-4703-ac18-0ed10970ae79"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[master 6e9d670] Initial commit: Added Jupyter notebooks\n"," 1 file changed, 1 insertion(+)\n"," create mode 100644 recursive.ipynb\n"]}]},{"cell_type":"code","source":["!git branch -M main\n","!git push https://<username>:{token}@github.com/<username>/<repo>.git main"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a7lIzsykr1Ab","executionInfo":{"status":"ok","timestamp":1755747199151,"user_tz":-480,"elapsed":313,"user":{"displayName":"Lakshminarayana Naidu","userId":"02129415072663551429"}},"outputId":"c8e3aea3-587e-4d51-eb29-23489efb877a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: The current branch main has no upstream branch.\n","To push the current branch and set the remote as upstream, use\n","\n","    git push --set-upstream origin main\n","\n"]}]},{"cell_type":"code","source":["git push --set-upstream origin main"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"D_rbRETCsGCJ","executionInfo":{"status":"error","timestamp":1755747216645,"user_tz":-480,"elapsed":15,"user":{"displayName":"Lakshminarayana Naidu","userId":"02129415072663551429"}},"outputId":"6e2ebfa7-97d3-4aff-f25f-a0d42f882690"},"execution_count":12,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (ipython-input-744213638.py, line 1)","traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-744213638.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    git push --set-upstream origin main\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]}]}